{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Model Implementations.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "poq_yOPPJHfk"
      },
      "source": [
        "# Loading Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZqSI7h1pqiN2",
        "outputId": "cce244b6-6b4a-4004-f6dc-5e28e5c0bd9f"
      },
      "source": [
        "from google.colab import auth\n",
        "import google.colab\n",
        "auth.authenticate_user()\n",
        "print('Authenticated')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Authenticated\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sAEPnoGtqs7V"
      },
      "source": [
        "%%bigquery --project formal-branch-269704 X_full\n",
        "SELECT * FROM `formal-branch-269704.home_credit_default.X_train`"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dU_IXoGb2xug"
      },
      "source": [
        "%%bigquery --project formal-branch-269704 y_full\n",
        "SELECT * FROM `formal-branch-269704.home_credit_default.y_train`"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bv707-M1oEbs"
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4pRGUc1hrK0o"
      },
      "source": [
        "Xfull = X_full.iloc[:,1:]\n",
        "Xfull['target'] = y_full['TARGET']\n",
        "\n",
        "# Dropping first column\n",
        "X_complete = X_full.iloc[:,1:]\n",
        "y_complete = y_full['TARGET']\n",
        "\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "scaler = StandardScaler()\n",
        "X_complete = scaler.fit_transform(X_complete)\n",
        "\n",
        "# To account for unbalanced data we train on balanced sub-samples\n",
        "class1 = Xfull[Xfull['target']==1].sample(10000)\n",
        "class0 = Xfull[Xfull['target']==-1].sample(10000)\n",
        "train = pd.concat([class1,class0])\n",
        "\n",
        "X = train.iloc[:,0:16]\n",
        "scaler = StandardScaler()\n",
        "X = scaler.fit_transform(X)\n",
        "y = train['target']\n",
        "\n",
        "n, p = X.shape\n",
        "\n",
        "# Setting regularization strength\n",
        "lam = 0.10819565\n",
        "\n",
        "# Target labels for logistic regression needs to be (0,1)\n",
        "y_logr = (y+1)/2"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cLm8KaCEJLWp"
      },
      "source": [
        "# Logistic Regression w/ Ridge Penalty"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8dNbyG4cwtSZ"
      },
      "source": [
        "**Objective Function for Logistic Regression with Ridge Penalty**\n",
        "$$ p_i = \\frac{e^{\\alpha+\\bf{x_i}ùõÉ}}{1+e^{\\alpha+\\bf{x_i}ùõÉ}}$$\n",
        "$$ L(y,p) = -y\\text{ log}(p) - (1-y) \\text{log} (1-p) + \\lambda \\sum_{j=1}^{p}\\beta^2_j $$\n",
        "$$ \\frac{\\partial J}{\\partial \\alpha} = -\\frac{1}{n} \\sum_{i=1}^n (y_i - p_i) $$\n",
        "$$ \\frac{\\partial J}{\\partial \\beta} = - \\frac{1}{n} \\sum_{i=1}^n (-y_i {\\bf x}_i^{T} + p_i  {\\bf x}_i^T) $$\n",
        "$$ \\frac{\\partial J}{\\partial \\beta} = - \\frac{1}{n} (-y_i X^{T} + p_i  X^T) $$"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gm2CPJoG1fcP",
        "outputId": "214eb7ed-74a1-4c33-fb6e-69aa08d89918"
      },
      "source": [
        "# Sklearn model for Logistic Regression with Ridge Penalty\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "clf = LogisticRegression(C=1/(2*n*lam))\n",
        "clf.fit(X,y_logr)\n",
        "clf.intercept_, clf.coef_"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(array([-0.03854883]),\n",
              " array([[-0.03752668,  0.01531697,  0.00656067, -0.03425379,  0.41324003,\n",
              "         -0.69639037, -0.02014574, -0.01225417,  0.00671573, -0.19576206,\n",
              "          0.01517515, -0.07876296,  0.0664365 ,  0.03174906, -0.01426948,\n",
              "          0.1004592 ]]))"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "abYFMUfD1bcy",
        "outputId": "adc38b38-23a5-44c4-86f4-07ec5bbda609"
      },
      "source": [
        "# Function for running Logistic Ridge Regression\n",
        "\n",
        "def logistic_reg(X, y, lr = .01, lam = lam):\n",
        "\n",
        "  n, p = X.shape\n",
        "\n",
        "  # Initializing values of alpha and beta to the origin\n",
        "  alpha = 0\n",
        "  betas = np.ones(p)\n",
        "\n",
        "  # Gradient Descent with Ridge Penalty\n",
        "  for _ in range(5000):\n",
        "\n",
        "    # Sigmoid Function\n",
        "    p_i = (np.exp(alpha+X@betas)) / (1 + np.exp(alpha+X@betas))\n",
        "\n",
        "    # Alpha gradient\n",
        "    grad_alpha = -(y - p_i).mean()\n",
        "    # Beta gradient\n",
        "    grad_betas = -(1/n)*((X.T@y) - (X.T@p_i)) + 2*lam*betas\n",
        "\n",
        "    # Taking steps in the opposite direction of the gradient\n",
        "    alpha = alpha - lr*grad_alpha\n",
        "    betas = betas - lr*grad_betas\n",
        "\n",
        "  return alpha, betas\n",
        "\n",
        "logr_alpha, logr_betas = logistic_reg(X, y_logr)\n",
        "logr_alpha, logr_betas"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(-0.038523121844216676,\n",
              " array([-0.03752474,  0.01531806,  0.00656113, -0.03425316,  0.41324359,\n",
              "        -0.69638376, -0.02014513, -0.01225363,  0.00671535, -0.19576242,\n",
              "         0.01517557, -0.07876255,  0.0664364 ,  0.03174901, -0.01426961,\n",
              "         0.10046109]))"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uYwlaSTPJPCy"
      },
      "source": [
        "# Support Vector Machines (SVM)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g5_SinfS5Yur"
      },
      "source": [
        "**Objective Function for SVM**\n",
        "\n",
        "$$\\displaystyle J(\\alpha,\\beta) = \\frac{1}{n}\\sum_{i=1}^n max(1-y_i(\\alpha+\\vec{x}_i\\vec{\\beta}),0) +\\lambda\\sum_{j=1}^p\\beta_j^2$$\n",
        "\n",
        "$$\\frac{\\partial J}{\\partial \\alpha} = \\frac{1}{n}\\sum_{i=1}^n\\begin{cases} \n",
        "      0 & 1-y_i(\\alpha+\\vec{x}_i\\vec{\\beta}) < 0 \\\\\n",
        "      -y_i & 1-y_i(\\alpha+\\vec{x}_i\\vec{\\beta}) >0 \n",
        "   \\end{cases}$$\n",
        "\n",
        "$$\\frac{\\partial J}{\\partial \\beta} = (\\frac{1}{n}\\sum_{i=1}^n\\begin{cases} \n",
        "      0 & 1-y_i(\\alpha+\\vec{x}_i\\vec{\\beta}) < 0 \\\\\n",
        "      -y_i\\vec{x}_i & 1-y_i(\\alpha+\\vec{x}_i\\vec{\\beta}) >0 \n",
        "   \\end{cases}) + 2\\lambda \\sum_{j=1}^p\\beta_i$$\n",
        "\n",
        "<br>\n",
        "\n",
        "Sci-kitlearn's implementation uses the objective function\n",
        "\n",
        "$$\\displaystyle J(\\alpha,\\beta) = C\\sum_{i=1}^n max(1-y_i(\\alpha+\\vec{x}_i\\vec{\\beta}),0) + \\frac{1}{2} \\sum_{j=1}^p\\beta_j^2$$\n",
        "\n",
        "If we modify our objective function\n",
        "\n",
        "$$\\displaystyle J(\\alpha,\\beta) = \\frac{1}{2n\\lambda}\\sum_{i=1}^n max(1-y_i(\\alpha+\\vec{x}_i\\vec{\\beta}),0) + \\frac{1}{2}\\sum_{j=1}^p\\beta_j^2$$\n",
        "\n",
        "We can see that $\\displaystyle C=\\frac{1}{2n\\lambda}$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aorxREKSdNZg"
      },
      "source": [
        "So after using hyperparameter tuning, we found that the lambda's would end up being similar for Perceptron and Logistic Regression. In fact, the actual decision boundaries were also very similar. However, for SVM it would be completely different. This makes sense because  the regularization term is for a different for SVM. So, we needed a different lambda for SVM to make it converge."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7lnoz6fYxWF3",
        "outputId": "9cbc6ef8-46a3-424d-d5eb-500583293ff0"
      },
      "source": [
        "# Sklearn model for SVM\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.pipeline import Pipeline\n",
        "\n",
        "clf = SVC(kernel='linear', C=1 / (2 * n * 1))\n",
        "clf.fit(X, y)\n",
        "clf.intercept_, clf.coef_"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(array([0.02552482]),\n",
              " array([[-0.03089084,  0.01904228,  0.00250013, -0.02380001,  0.29342944,\n",
              "         -0.3695291 , -0.0069959 , -0.00443586,  0.00275573, -0.11035444,\n",
              "          0.01716461, -0.02613729,  0.05060491,  0.0214118 , -0.01206518,\n",
              "          0.06225573]]))"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "woShdHpvxbh9",
        "outputId": "75049f0c-b7e4-46b8-8873-c876b66cc091"
      },
      "source": [
        "# My SVM implementation\n",
        "import numpy as np\n",
        "\n",
        "# initialize alpha and betas\n",
        "alpha = 0\n",
        "betas = np.ones(p)\n",
        "\n",
        "#setting learning rate\n",
        "learning_rate = 0.01\n",
        "\n",
        "for _ in range(5000):\n",
        "    # Coefficient penalty term\n",
        "    penalty = 2 * 1 * betas\n",
        "    # Checking if y is on correct side of margin or not\n",
        "    incorrect_map = ((1 - y * (alpha + X @ betas)) > 0)\n",
        "\n",
        "    # alpha gradient is the sum of -y for every y that is on \n",
        "    # the incorrect side of the margin divided by n\n",
        "    d_alpha = 1/n * -y @ incorrect_map\n",
        "\n",
        "    # beta gradient is sum of -y * x for every y that is on the incorrect side\n",
        "    # of the margin divided by n + coefficient penalty term\n",
        "    d_beta = 1/n * -y * incorrect_map @ X + penalty\n",
        "\n",
        "    # Take steps in the opposite direction of the gradient\n",
        "    alpha -= learning_rate * d_alpha\n",
        "    betas -= learning_rate * d_beta\n",
        "\n",
        "svm_alphas, svm_betas = alpha, betas\n",
        "svm_alphas, svm_betas"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(0.02562250000000107,\n",
              " array([-0.03089075,  0.01902537,  0.00250013, -0.02378231,  0.29341174,\n",
              "        -0.36951094, -0.00695738, -0.00442313,  0.00280566, -0.1103313 ,\n",
              "         0.0171829 , -0.02613464,  0.05057741,  0.02140252, -0.01204823,\n",
              "         0.06224347]))"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tsAb57BhJT0e"
      },
      "source": [
        "# Perceptron\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Gh3KCbplCf0g"
      },
      "source": [
        "**Objective Function for Perceptron**\n",
        "\n",
        "$$ J(\\alpha, \\boldsymbol \\beta) = \\frac{1}{n} \\sum_{i=1}^{n} (y_i - \\alpha -  \\boldsymbol{x_i\\beta})^2 $$\n",
        "\n",
        "$$ \\frac{\\partial J}{\\partial \\alpha} = \\frac{1}{n} \\sum_{i=1}^{n} -2(y_i - \\alpha - \\boldsymbol{x_i\\beta}) $$\n",
        "\n",
        "$$ \\alpha = \\bar y - \\bar X \\boldsymbol{\\beta} $$\n",
        "\n",
        "$$ \\frac{\\partial J}{\\partial \\beta} = \\frac{1}{n} \\sum_{i=1}^{n} 2 (y_i - \\alpha -  x_i \\boldsymbol{\\beta}) (-\\boldsymbol{x_i^T})$$\n",
        "\n",
        "$$ \\beta = (\\tilde X^T \\tilde X)^{-1} \\tilde X ^T \\boldsymbol{\\tilde y}$$\n",
        "\n",
        "Where $\\tilde X$ and $\\tilde y$ represent the centered $X$'s and centered $y$'s, respectively"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W6OrCqka1owB",
        "outputId": "6d81759d-5ef5-41ba-b721-662f8cab8989"
      },
      "source": [
        "# Sklearn ridge classifier implementation (Perceptron w/ Ridge penalty)\n",
        "from sklearn.linear_model import RidgeClassifier\n",
        "\n",
        "clf = RidgeClassifier(alpha= lam * n, solver='lsqr')\n",
        "clf.fit(X,y)\n",
        "clf.intercept_, clf.coef_"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(array([1.04143196e-16]),\n",
              " array([[-1.93479563e-03, -3.10344687e-03,  2.19312877e-03,\n",
              "         -1.04603033e-02,  1.14807609e-01, -5.98235352e-01,\n",
              "         -5.05457379e-03, -3.63996704e-03,  2.24734384e-03,\n",
              "         -1.02830702e-01, -4.78831926e-04, -4.69657928e-02,\n",
              "          7.37622727e-03,  1.18709193e-02, -5.55994216e-04,\n",
              "          3.11265812e-02]]))"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b8-xTbPc1lTh",
        "outputId": "3105f30a-8f03-4b99-d28d-6dd8f633c1b0"
      },
      "source": [
        "# Function for running Perceptron with a ridge penalty\n",
        "\n",
        "def perceptron(X, y, lam=lam, lr=0.1):\n",
        "\n",
        "  # initialize alpha and betas to the origin\n",
        "  alpha = 0\n",
        "  betas = np.ones(p)\n",
        "\n",
        "  # Finding the means for X and y\n",
        "  y_mean = y.mean()\n",
        "  X_mean = X.mean()\n",
        "\n",
        "  # Gradient Descent with Ridge penalty\n",
        "  for _ in range(5000):\n",
        "    # Alpha Gradient\n",
        "    d_alpha = -2 * (y_mean - alpha - (X_mean * betas).sum())\n",
        "    # Beta Gradient\n",
        "    d_beta = -2 / n * X.T @ (y - alpha - X @ betas) + 2*lam*betas\n",
        "    # Taking steps in the opposite direction of the gradient\n",
        "    alpha -= lr * d_alpha\n",
        "    betas -= lr * d_beta\n",
        "\n",
        "  return alpha, betas\n",
        "\n",
        "perc_alpha, perc_betas = perceptron(X, y)\n",
        "perc_alpha, perc_betas"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(1.2169513989826738e-17,\n",
              " array([-2.72954145e-03, -3.02641854e-03,  2.08634337e-03, -1.10701660e-02,\n",
              "         1.14794997e-01, -5.98271718e-01, -4.67684345e-03, -3.74946156e-03,\n",
              "         1.92311604e-03, -1.02747869e-01, -4.88957386e-04, -4.71597585e-02,\n",
              "         7.10304012e-03,  1.14845264e-02, -5.60290871e-04,  3.08210968e-02]))"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "He2u5mVYphzO"
      },
      "source": [
        "# Functions to make predictions for each model\n",
        "def make_predictions(X, alpha, betas, logreg=False):\n",
        "  predictions = []\n",
        "  for i in range(len(X)):\n",
        "    # Adding the intercept term to the prediction first\n",
        "    y_hat = alpha\n",
        "    # Looping through each beta coefficient and adding the product of the \n",
        "    # coefficient and its corresponding x value to the prediction\n",
        "    for j in range(len(betas)):\n",
        "      y_hat += X[i,j]*betas[j]\n",
        "      # Classifying values based on their y-hat values\n",
        "    if logreg:\n",
        "      # For logistic regression we have to pass the prediction through the\n",
        "      # sigmoid function before classifying the value\n",
        "      y_hat = np.exp(y_hat)/(1+np.exp(y_hat))\n",
        "      if y_hat >= 0.5:\n",
        "        pred = 1\n",
        "      else:\n",
        "        pred = 0\n",
        "    else:\n",
        "      if y_hat >= 0:\n",
        "        pred = 1\n",
        "      else:\n",
        "        pred = -1\n",
        "    # Appending predictions for each row to a list\n",
        "    predictions.append(pred)\n",
        "  return predictions"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fqU7S80hqvU_"
      },
      "source": [
        "logr_preds = make_predictions(X_complete, logr_alpha, logr_betas, logreg=True)\n",
        "perc_preds = make_predictions(X_complete, perc_alpha, perc_betas)\n",
        "svm_preds = make_predictions(X_complete, svm_alphas, svm_betas)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tJMrfiOH9WX3",
        "outputId": "42fd2c33-2635-4f7e-805f-0a66f823c0c3"
      },
      "source": [
        "logr_y_complete = (y_complete+1)/2\n",
        "\n",
        "from sklearn.metrics import f1_score\n",
        "logr_f1 = f1_score(logr_y_complete, logr_preds, pos_label=1, average='binary')\n",
        "svm_f1 = f1_score(y_complete, svm_preds, pos_label=1, average='binary')\n",
        "perc_f1 = f1_score(y_complete, perc_preds, pos_label=1, average='binary')\n",
        "\n",
        "logr_f1, svm_f1, perc_f1"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(0.3963236942389599, 0.3914907077896402, 0.4332157765312284)"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TgbX_QDHfPm9",
        "outputId": "5ab9d053-27cc-44f0-8982-eadf499edf0d"
      },
      "source": [
        "from sklearn.metrics import accuracy_score\n",
        "logr_acc = accuracy_score(logr_y_complete, logr_preds)\n",
        "svm_acc = accuracy_score(y_complete, svm_preds)\n",
        "perc_acc = accuracy_score(y_complete, perc_preds)\n",
        "\n",
        "logr_acc, svm_acc, perc_acc"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(0.754033441848631, 0.7490034642258336, 0.788731153893829)"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FeJ2kCwsqb4J"
      },
      "source": [
        "preds1 = pd.Series(logr_preds)*2-1\n",
        "preds2 = pd.Series(svm_preds)\n",
        "preds3 = pd.Series(perc_preds)\n",
        "\n",
        "preds = pd.DataFrame({'LogReg':preds1, 'SVM':preds2, 'Perceptron':preds3})\n",
        "preds['EXT_SOURCE_2'] = Xfull['remainder__EXT_SOURCE_2']\n",
        "preds = pd.melt(preds, id_vars=['EXT_SOURCE_2'], value_vars=['LogReg', 'SVM', 'Perceptron'])"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}